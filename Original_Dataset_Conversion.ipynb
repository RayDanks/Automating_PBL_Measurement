{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Original_Dataset_Conversion.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation\n",
        "\n",
        "This is the first script run.\n",
        "\n",
        "The data originally came as full periapical radiographs, which contain 3-4 teeth typically. This code is to separate them into individual teeth and classify them based on their roots.\n",
        "\n",
        "The clinicians (A Orishko and H J Tan) labelled the landmarks, type of tooth, bounding box and root morphology of each individual tooth.\n",
        "\n",
        "As is expected with large labelling-based projects, there were some errors in the labelling. One of the key functions of this script is to identify these errors and exclude them from the processed dataset."
      ],
      "metadata": {
        "id": "OqYKeEDp8wNf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6dYbaDRnHHS"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n1BDQ5CItg_"
      },
      "source": [
        "!unzip '/content/drive/My Drive/Labelled_Data.zip' #placeholder for github"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHNg9cqePhc3"
      },
      "source": [
        "!mkdir 'individual_teeth'\n",
        "!mkdir 'individual_teeth/single_roots'\n",
        "!mkdir 'individual_teeth/double_roots'\n",
        "!mkdir 'individual_teeth/triple_roots'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import sys\n",
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "from google.colab.patches import cv2_imshow #specifically for collab\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "mPkh-Cgv-nfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXK4DQ8OQsfy"
      },
      "source": [
        "def add_smart_buffer(image,bbox,buffer): #adds a buffer to the cropped image that does not go outside of the image boundary (causing errors)\n",
        "\n",
        "  img_height, img_width,_ = image.shape\n",
        "\n",
        "  x, y, width, height = bbox\n",
        "  buffered_x = x - buffer\n",
        "\n",
        "  if buffered_x < 0:\n",
        "    buffered_x = 0\n",
        "\n",
        "  buffered_y = y - buffer\n",
        "\n",
        "  if buffered_y < 0:\n",
        "    buffered_y = 0\n",
        "\n",
        "  buffered_height = height + 2*buffer #2 buffers, one for top and one for bottom (to adjust for the x, y change)\n",
        "\n",
        "  if buffered_height + buffered_y > img_height:\n",
        "    buffered_height = img_height - buffered_y #maximum it can be without going off image\n",
        "\n",
        "  buffered_width = width + 2*buffer\n",
        "\n",
        "  if buffered_width + buffered_x > img_width:\n",
        "    buffered_width = img_width - buffered_x\n",
        "\n",
        "  return (buffered_x,buffered_y,buffered_width,buffered_height)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74C5z_rJUvDN"
      },
      "source": [
        "def add_regularisation_border(image,desired_width,desired_height,landmarks,image_path):\n",
        "  #add a border to an image such that it becomes the same size as is desired\n",
        "\n",
        "  \"\"\"\n",
        "  Assistance: https://stackoverflow.com/questions/43391205/add-padding-to-images-to-get-them-into-the-same-shape\n",
        "  \"\"\"\n",
        "\n",
        "  img_height, img_width, img_channels = image.shape #channels should be constant\n",
        "  color = (0,0,0) #black border\n",
        "  new_image = np.full((desired_height,desired_width,img_channels), color, dtype=np.int32) #just creates a block coloured image of desired size.\n",
        "  \n",
        "  #centre offset\n",
        "  xx = (desired_width - img_width) // 2\n",
        "  yy = (desired_height - img_height) // 2\n",
        "\n",
        "  # copy original image into centre of block-coloured image\n",
        "  new_image[yy:yy+img_height, xx:xx+img_width] = image\n",
        "\n",
        "  #As an extra, we need to adjust the landmarks such that they are not thrown off by the additional image size\n",
        "  image_name = str.split(image_path,'/')[-1]\n",
        "  image_name = str.split(image_name,'.')[0]\n",
        "  try:\n",
        "    current_landmarks = landmarks[image_name]\n",
        "  except KeyError:\n",
        "    print(\"THIS IMAGE HAS NO LANDMARK DATA!!!!!!!!!!!!!!!\",image_name)\n",
        "\n",
        "    return None, landmarks, 1 #final value is the error decider\n",
        " \n",
        "  #now adjust the landmarks\n",
        "  for i, point in enumerate(current_landmarks):\n",
        "\n",
        "    landmarks[image_name][i][0] += xx\n",
        "    landmarks[image_name][i][1] += yy\n",
        "\n",
        "  return new_image, landmarks, 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rowB3Qq58Gu"
      },
      "source": [
        "def Count_Dictionary_Values(dictionary):\n",
        "  #check how many keys in a dict have the same value\n",
        "  new_dict = {}\n",
        "  for key,value in dictionary.items():\n",
        "    if value not in new_dict:\n",
        "      new_dict[value] = 1\n",
        "    else:\n",
        "      new_dict[value] += 1\n",
        "  return new_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zRNQ8L4qNSS"
      },
      "source": [
        "#We have multiple jsons and hence w emust go through them iteratively.\n",
        "\n",
        "list_of_jsons = glob.glob('/content/New_Databases_August/Raw_Data/*.json')\n",
        "%cd '/content/New_Databases_August/Raw_Data/images'\n",
        "\n",
        "!mkdir 'individual_teeth'\n",
        "!mkdir 'individual_teeth/single_roots'\n",
        "!mkdir 'individual_teeth/double_roots'\n",
        "!mkdir 'individual_teeth/triple_roots'\n",
        "\n",
        "error_x_rays = {}\n",
        "bboxes = {}\n",
        "landmarks = {}\n",
        "insufficient_landmarks = {}\n",
        "for json_path in list_of_jsons:\n",
        "  with open(json_path) as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "  try:\n",
        "    image_list = data['_via_img_metadata']\n",
        "  except KeyError: #the jsons are not in the same format\n",
        "    image_list = data\n",
        "\n",
        "  teeth_in_image = 0\n",
        "  amount_of_unassigned_points = 0\n",
        "  non_tooth_ids = 0\n",
        "  buffer = 25 #buffer for the bounding boxes\n",
        "\n",
        "  #examples of errors below:\n",
        "\n",
        "  # example_image = image_list['002 copy.jpg111200'] #IMPORTANT: This shows the error oin the bounding box in the dataset\n",
        "  # example_image = image_list['051 copy 8.jpg106811'] #IMPORTANT: Single_2 on this image only has one landmark - confirmed in data too\n",
        "\n",
        "  for _,example_image in image_list.items():\n",
        "    image_name = example_image['filename']\n",
        "    full_img = img = cv2.imread(image_name)\n",
        "\n",
        "    example_image_regions = example_image['regions']\n",
        "    example_image_attributes = example_image['file_attributes']\n",
        "\n",
        "    #Find the bone level to make a proper mask!\n",
        "    for datapoint in example_image_regions:\n",
        "      shape_attr = datapoint['shape_attributes'] #contains bounding box and point locations\n",
        "      region_attr = datapoint['region_attributes']\n",
        "\n",
        "      if not region_attr:\n",
        "        continue\n",
        "      elif 'Toothtype' not in region_attr:\n",
        "        continue #unassigned points ?\n",
        "      tooth_type = region_attr['Toothtype']\n",
        "      # print(\"tooth_type\",tooth_type)\n",
        "      if tooth_type == 'Bone_region':\n",
        "        try:\n",
        "          x_points = shape_attr['all_points_x']\n",
        "        except KeyError:\n",
        "          error_x_rays[image_name] = \"Labelled bone level but isn't\" #wrong tooth type - labelled bone level but it's not\n",
        "          continue\n",
        "        y_points = shape_attr['all_points_y']\n",
        "\n",
        "        for i in range(len(x_points)):\n",
        "          current_x_y = [[x_points[i],y_points[i]]] #the points / contours must be in this form\n",
        "          if i == 0:\n",
        "            bone_level_points = current_x_y\n",
        "          else:\n",
        "            bone_level_points = np.vstack((bone_level_points,current_x_y))\n",
        "\n",
        "    for datapoint in example_image_regions:\n",
        "      shape_attr = datapoint['shape_attributes'] #contains bounding box and point locations\n",
        "      region_attr = datapoint['region_attributes'] #contains tooth type and tooth id\n",
        "\n",
        "      if not region_attr: #if dict is empty\n",
        "        amount_of_unassigned_points += 1\n",
        "        #Some of the datapoints don't have any \"single\" or \"double\" root etc,\n",
        "        #there are just arbitrary points with no assignment\n",
        "        continue\n",
        "\n",
        "      elif 'Tooth_ID' not in list(region_attr.keys()):\n",
        "        #this is common and expected when the \"bone region\" is labelled\n",
        "        continue\n",
        "\n",
        "      tooth_ID = region_attr['Tooth_ID'] #assigns this particular datapoint to a certain tooth\n",
        "      tooth_type = str.split(tooth_ID,'_')[0]#single, double or triple - for saving into the right folder!\n",
        "\n",
        "      \"\"\"\n",
        "      So what we would like to do is crop the images so that each one only\n",
        "      contains one tooth firstly. After this, creating a simple dictionary\n",
        "      (to preface a dataframe) which has each image and tooth as a key and their\n",
        "      data laid out well would be good.\n",
        "\n",
        "      Help with bounding box cropping: \n",
        "      https://stackoverflow.com/questions/48301186/cropping-concave-polygon-from-image-using-opencv-python\n",
        "      \"\"\"\n",
        "      datapoint_name = shape_attr['name']\n",
        "\n",
        "      \"\"\"\n",
        "      Cropping and masking the images\n",
        "      \"\"\"\n",
        "      \n",
        "      if datapoint_name == 'polygon' or datapoint_name == 'polyline': #begin the cropping process\n",
        "\n",
        "        teeth_in_image += 1 #this should be printed after each image analysis, for manual checks\n",
        "\n",
        "        #firstly turn the \"all points x\" and \"all points y\" into real point array\n",
        "        all_x_points = shape_attr['all_points_x']\n",
        "        all_y_points = shape_attr['all_points_y']\n",
        "        amount_of_bbox_points = len(all_x_points) #same size as y points\n",
        "\n",
        "        for i in range(amount_of_bbox_points):\n",
        "          current_x_y = [[all_x_points[i],all_y_points[i]]] #the points / contours must be in this form\n",
        "          if i == 0:\n",
        "            regularised_bbox_points = current_x_y\n",
        "          else:\n",
        "            regularised_bbox_points = np.vstack((regularised_bbox_points,current_x_y))  \n",
        "\n",
        "        #Next: Establish the rectangular bbox as images must be rectangular (img dimensions)\n",
        "        rect_bbox = cv2.boundingRect(regularised_bbox_points)\n",
        "\n",
        "        new_rect_bbox = add_smart_buffer(full_img,rect_bbox,buffer)\n",
        "        x,y,width,height = new_rect_bbox #REASSIGN!\n",
        "        #x,y is \"bottom left\" corner and width and height can find all other corners\n",
        "\n",
        "        \"\"\"\n",
        "        It may be more useful to \"black out\" any of the teeth which are not\n",
        "        the tooth in question - whilt retaining the bone level !\n",
        "        mask both tooth and bone level before cropping!\n",
        "        \"\"\"\n",
        "\n",
        "        #two separate masks below\n",
        "        tooth_mask = np.zeros(full_img.shape[:2], np.uint8)\n",
        "        cv2.drawContours(tooth_mask, [regularised_bbox_points], -1, (255, 255, 255), -1, cv2.LINE_AA)\n",
        "\n",
        "        bone_mask = np.zeros(full_img.shape[:2], np.uint8)\n",
        "        cv2.drawContours(tooth_mask, [bone_level_points], -1, (255, 255, 255), -1, cv2.LINE_AA)\n",
        "\n",
        "\n",
        "        tooth_and_bone_mask = tooth_mask + bone_mask #overall mask\n",
        "\n",
        "        # masked_tooth = full_img\n",
        "        masked_tooth = cv2.bitwise_and(full_img, full_img, mask=tooth_and_bone_mask)\n",
        "\n",
        "        cropped_single_tooth = masked_tooth[y:y+height, x:x+width].copy() #crop the masked version. not full_img\n",
        "        #now we have a copy of a single cropped image of one tooth\n",
        "\n",
        "        #Now we have succesffuly cropped the tooth!\n",
        "\n",
        "        #If the bounding box only outlines the tooth, then adding a mask would\n",
        "        #get rid of the bone segment - I should consult Sophia & Anastasiya about\n",
        "        #whether or not this could hiner the network (perhaps show both options?)\n",
        "        \n",
        "\n",
        "        #For saving, firstly let's create a new name for the singular tooth image\n",
        "        #Just the old name with the tooth_ID appended\n",
        "        image_name = str.split(image_name,'.')[0] # get rid of the .jpg\n",
        "        new_name = image_name + '_' + tooth_ID + '.jpg'\n",
        "\n",
        "        bboxes[str.split(new_name,'.')[0]] = new_rect_bbox #need to record bboxes for the point translation\n",
        "        if tooth_type == 'Single':\n",
        "          cv2.imwrite('individual_teeth/single_roots/' + new_name, cropped_single_tooth)\n",
        "        elif tooth_type == 'Double':\n",
        "          cv2.imwrite('individual_teeth/double_roots/' + new_name, cropped_single_tooth)\n",
        "        elif tooth_type == 'Triple':\n",
        "          cv2.imwrite('individual_teeth/triple_roots/' + new_name, cropped_single_tooth)\n",
        "        else:\n",
        "          raise Exception(\"Unidentified Tooth Type\")\n",
        "      teeth_in_image -= 1 #one of the polygons is bone\n",
        "\n",
        "  #if datapoint isn't polygon - it should be a point - loop through again since we need all bboxes first!!\n",
        "    for datapoint in example_image_regions:\n",
        "      shape_attr = datapoint['shape_attributes'] #contains bounding box and point locations\n",
        "      region_attr = datapoint['region_attributes'] #contains tooth type and tooth id\n",
        "\n",
        "      if not region_attr:\n",
        "        amount_of_unassigned_points += 1\n",
        "        continue\n",
        "      elif 'Tooth_ID' not in list(region_attr.keys()):\n",
        "        non_tooth_ids += 1\n",
        "        continue\n",
        "\n",
        "      tooth_ID = region_attr['Tooth_ID'] \n",
        "      tooth_type = str.split(tooth_ID,'_')[0]#single, double or triple\n",
        "      datapoint_name = shape_attr['name']\n",
        "\n",
        "      if datapoint_name != 'polygon' and datapoint_name != 'polyline': #points, not polygons\n",
        "\n",
        "        \"\"\"\n",
        "        Creating the CSV to show the points on the cropped images\n",
        "\n",
        "        I have spent a little bit of time investigating in what form to create my dataset,\n",
        "        however either way, when it comes to training the set would need to be flattened\n",
        "        out and regressed upon, therefore we may as well just flatten it out and put it into\n",
        "        a CSV. Usually distributable datasets are in .pts format bt this is not entirely necessary here.\n",
        "        \"\"\"\n",
        "\n",
        "        #start it off with a dictionary with the teeth names in.\n",
        "        image_name = str.split(image_name,'.')[0] # get rid of the .jpg\n",
        "        new_name = image_name + '_' + tooth_ID\n",
        "\n",
        "        # if (new_name == '003_Single_1' or new_name == '009 copy 5_Single_2' or new_name == '009 copy_Single_1' #IMPORTANT: ERRORS IN DATASET\n",
        "        # or new_name == '029 copy 6_Double_2'):\n",
        "        #   print(\"ERROR!!!!!!!!!!!!\")\n",
        "        #   continue\n",
        "        # print(\"datapoint name\",datapoint_name)\n",
        "        x_point = shape_attr['cx'] #x value of this paticular point\n",
        "        y_point = shape_attr['cy'] #y value of this paticular point\n",
        "\n",
        "        try:\n",
        "          tooth_bbox = bboxes[new_name] #the bounding box should always be calculated before the point analysis!\n",
        "        except KeyError:\n",
        "          error_x_rays[new_name] = \"no bbox\" #no bounding box\n",
        "          continue\n",
        "        # print(\"tooth bbox\",tooth_bbox)\n",
        "        bbox_x,bbox_y,_,_ = tooth_bbox\n",
        "        new_x_point = x_point - bbox_x #x point after accounting for the cropped image !!\n",
        "        new_y_point = y_point - bbox_y #y point after accounting for the cropped image !!\n",
        "        \n",
        "\n",
        "\n",
        "        #find the current image to print the point on it:\n",
        "        # print(\"new name\",new_name)\n",
        "        if tooth_type == 'Single':\n",
        "          current_tooth = cv2.imread('individual_teeth/single_roots/' + new_name + '.jpg') #TODO: HOW DOES THIS WORK IF THE IMAGE IS A .TIF ??\n",
        "        elif tooth_type == 'Double':\n",
        "          current_tooth = cv2.imread('individual_teeth/double_roots/' + new_name + '.jpg')\n",
        "        elif tooth_type == 'Triple':\n",
        "          current_tooth = cv2.imread('individual_teeth/triple_roots/' + new_name + '.jpg')\n",
        "\n",
        "        show_points = cv2.circle(current_tooth, (new_x_point,new_y_point), 4, color=(0, 0, 255))\n",
        "        # cv2_imshow(show_points)\n",
        "\n",
        "        \"\"\"\n",
        "        Determine the type of the landmark!\n",
        "        \"\"\"\n",
        "\n",
        "        if tooth_type == 'Single':\n",
        "          try:\n",
        "            landmark_type = region_attr['Single_root']\n",
        "          except KeyError:\n",
        "            landmark_type = None #this means that there is an error \n",
        "        elif tooth_type == 'Double':\n",
        "          try: \n",
        "            landmark_type = region_attr['Double_root']\n",
        "          except KeyError:\n",
        "            landmark_type = None #this means that there is an error \n",
        "        elif tooth_type == 'Triple':\n",
        "          try: \n",
        "            landmark_type = region_attr['Triple_root']\n",
        "          except KeyError:\n",
        "            landmark_type = None #this means that there is an error \n",
        "        else:\n",
        "          print(\"ERROR ON TOOTH TYPE\")\n",
        "          break\n",
        "\n",
        "        if new_name not in list(landmarks.keys()) and new_name not in list(error_x_rays.keys()):\n",
        "          \"\"\"\n",
        "          In order to posit a discrete order of landmarks, we should first create a\n",
        "          system whereby all of the landmarks have their own order, and it can be identified\n",
        "          if they're not in that order\n",
        "          \"\"\"\n",
        "          #create the placeholder\n",
        "\n",
        "          if tooth_type == 'Single':\n",
        "            filling_matrix = np.empty((ideal_landmarks_single,2))\n",
        "            filling_matrix.fill(np.nan) #creates a matrix full of NaN results\n",
        "            landmarks[new_name] = filling_matrix\n",
        "          elif tooth_type == 'Double':\n",
        "            filling_matrix = np.empty((ideal_landmarks_double,2))\n",
        "            filling_matrix.fill(np.nan) #creates a matrix full of NaN results\n",
        "            landmarks[new_name] = filling_matrix\n",
        "          elif tooth_type == 'Triple':\n",
        "            filling_matrix = np.empty((ideal_landmarks_triple,2))\n",
        "            filling_matrix.fill(np.nan) #creates a matrix full of NaN results\n",
        "            landmarks[new_name] = filling_matrix\n",
        "          else:\n",
        "            print(\"ERROR ON TOOTH TYPE\")\n",
        "            break\n",
        "\n",
        "        if landmark_type == None: #there is a faulty landmark so the data can't be properly analysed\n",
        "          error_x_rays[new_name] = \"landmark type is none\"\n",
        "          if new_name in list(landmarks.keys()): #the same tooth can have multiple labelling errors\n",
        "            del landmarks[new_name]\n",
        "\n",
        "        else:\n",
        "\n",
        "          \"\"\"\n",
        "          Now that we have made sure that all of the teeth have landmark placeholders,\n",
        "          we can fill them out.\n",
        "          \"\"\"\n",
        "          new_point = [new_x_point]\n",
        "          new_point.append(new_y_point)\n",
        "          if tooth_type == 'Single':\n",
        "            landmark_order = single_landmark_order[landmark_type]\n",
        "          elif tooth_type == 'Double':\n",
        "            try: \n",
        "              landmark_order = double_landmark_order[landmark_type]\n",
        "            except KeyError:\n",
        "              #IMPORTANT: there are some incorrectly labelled x-rays and hence these cannot be analysed... they are labelled as both single and double\n",
        "              error_x_rays[new_name] = \"error ??\" #so that these can be ignored (and hopefully fixed at a later date ??)\n",
        "              if new_name in list(landmarks.keys()): #the same tooth can have multiple labelling errors\n",
        "                del landmarks[new_name]\n",
        "              # sys.exit()\n",
        "          elif tooth_type == 'Triple':\n",
        "            landmark_order = triple_landmark_order[landmark_type]\n",
        "          else:\n",
        "            print(\"ERROR ON TOOTH TYPE\")\n",
        "            break\n",
        "        \n",
        "        if new_name not in list(error_x_rays.keys()):\n",
        "          landmarks[new_name][landmark_order] = new_point\n",
        "\n",
        "\"\"\"\n",
        "Now we must make sure that all images are the same size - thus resave them!\n",
        "we need to loop through all of the cropped images, find their size and regularise them\n",
        "\"\"\"\n",
        "image_list_single = glob.glob('individual_teeth/single_roots/*.jpg')\n",
        "image_list_double = glob.glob('individual_teeth/double_roots/*.jpg')\n",
        "image_list_triple = glob.glob('individual_teeth/triple_roots/*.jpg')\n",
        "\n",
        "image_list = image_list_single\n",
        "image_list.extend(image_list_double)\n",
        "image_list.extend(image_list_triple)\n",
        "\n",
        "for i, image_path in enumerate(image_list): #delete the teeth with labelling errors\n",
        "  image_name = str.split(image_path,'/')[-1]\n",
        "  image_name = str.split(image_name,'.')[0]\n",
        "  if image_name in list(error_x_rays.keys()):\n",
        "    del image_list[i]\n",
        "\n",
        "max_height = 0 #instantiation\n",
        "max_width = 0\n",
        "for image_path in image_list:\n",
        "  original_img = cv2.imread(image_path)\n",
        "  img_height, img_width,_ = original_img.shape\n",
        "  if img_height > max_height:\n",
        "    max_height = img_height\n",
        "  if img_width > max_width:\n",
        "    max_width = img_width\n",
        "\n",
        "for image_path in image_list:\n",
        "  # sys.exit()\n",
        "  img = cv2.imread(image_path)\n",
        "  img_name = str.split(image_path,'/')[-1]\n",
        "  img_name = str.split(img_name,'.')[0]\n",
        "  try:\n",
        "    landmark_list = landmarks[img_name] #shape: number of landmarks x 2\n",
        "  except KeyError:\n",
        "    error_x_rays[img_name] = \"no landmarks\"\n",
        "    continue\n",
        "  for point in landmark_list: #point is 2-length list\n",
        "    if point[0] != point[0]: #great way to check for NaNs\n",
        "      insufficient_landmarks[img_name] = img_name\n",
        "      continue #this shows that the image does not have all the required landmarks\n",
        "\n",
        "    current_x = int(point[0]) #todo: change landmark array form to int instead of float - would this compromise anything ?\n",
        "    current_y = int(point[1])\n",
        "    visualise_img = cv2.circle(img, (current_x,current_y), 4, color=(0, 0, 255))\n",
        "\n",
        "  print(img_name,\":\")\n",
        "  cv2_imshow(visualise_img)\n",
        "\n",
        "amount_of_teeth = len(list(landmarks.keys()))\n",
        "amount_of_errors = len(list(error_x_rays.keys()))\n",
        "\n",
        "# print(\"landmarks: \",landmarks)\n",
        "print(\"landmarks keys\",list(landmarks.keys()))\n",
        "print(\"error x rays\",error_x_rays)\n",
        "print(\"amount of error x rays\",amount_of_errors)\n",
        "print(\"amount of usable x rays\",amount_of_teeth)\n",
        "print(\"percentage of error x rays: \",(amount_of_errors*100)/(amount_of_errors+amount_of_teeth),\"%\")\n",
        "print(\"types of errors\",Count_Dictionary_Values(error_x_rays))\n",
        "print(\"amount_of_unassigned_points\",amount_of_unassigned_points)\n",
        "print(\"non_tooth_ids\",non_tooth_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaaebJkj17M4"
      },
      "source": [
        "Now I use the landmarks dictionary to make CSVs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngu8VwZe2AFM"
      },
      "source": [
        "#we need 3 CSVs - for single, double and triple teeth, since they have different numbers of landmarks\n",
        "single_counter = 0 #instantiations\n",
        "double_counter = 0\n",
        "triple_counter = 0\n",
        "\n",
        "single_correct_landmarks = 0\n",
        "double_correct_landmarks = 0\n",
        "triple_correct_landmarks = 0\n",
        "V = ideal_landmarks_single\n",
        "single_correct_length = 5*2 + 1\n",
        "double_correct_length = 8*2 + 1 \n",
        "triple_correct_length = 9*2 + 1 \n",
        "\n",
        "#manually seen from a look through the above visualised radiographs\n",
        "visually_incorrect_labels = ['009 copy 5_Single_1', '051 copy 8_Single_1',\n",
        "                             '051 copy 7_Single_1', '051 copy 7_Single_2',\n",
        "                             '015 copy 2_Single_1']\n",
        "\n",
        "for i, (key, value) in enumerate(landmarks.items()):\n",
        "  \n",
        "  \"\"\"\n",
        "  So from visual inspection, there are more labelling database errors which involve\n",
        "  some teeth being labelled by their boundign box as one tooth, but the landmarks are assigned\n",
        "  to another - these should be excluded from the training set as they could heavily jeopardise\n",
        "  the training.\n",
        "  \"\"\"\n",
        "  if key in visually_incorrect_labels:\n",
        "    print(key,\"has errors in its labelling and hence is being disregarded\")\n",
        "    error_x_rays[key] = \"visually incorrect\"\n",
        "    continue #just skip this one\n",
        "\n",
        "  #currently the goal is to disregard all images which do not have the desired\n",
        "  #number of landmarks annotated - therefore, they will simply not be included in the csv\n",
        "  all_landmarks_present = 1\n",
        "  for point in value:\n",
        "    if point[0] != point[0] or point[1] != point[1]: #checking for NaN\n",
        "      all_landmarks_present = 0\n",
        "  \n",
        "  if all_landmarks_present == 0:\n",
        "    error_x_rays[key] = \"not all landmarks labelled\"\n",
        "    continue #skip the rest of this loop - do not record the tooth image data\n",
        "\n",
        "  root_type = str.split(key,'_')[-2]\n",
        "\n",
        "  key_list = [key]\n",
        "  value = np.concatenate(value).ravel().tolist() #full of numpy arrays - unravelling them.\n",
        "  key_list.extend(value)\n",
        "  csv_row = np.array(key_list)\n",
        "\n",
        "  if root_type == 'Single':\n",
        "    single_counter += 1\n",
        "    if single_counter == 1:\n",
        "      csv_single_matrix = csv_row\n",
        "    else:\n",
        "      csv_single_matrix = np.vstack((csv_single_matrix,csv_row))\n",
        "\n",
        "\n",
        "    if len(csv_row) == single_correct_length:\n",
        "      single_correct_landmarks += 1\n",
        "  if root_type == 'Double':\n",
        "    double_counter += 1\n",
        "    if double_counter == 1:\n",
        "      csv_double_matrix = csv_row\n",
        "    else:\n",
        "      csv_double_matrix = np.vstack((csv_double_matrix,csv_row))\n",
        "\n",
        "    \n",
        "    if len(csv_row) == double_correct_length:\n",
        "      double_correct_landmarks += 1\n",
        "\n",
        "\n",
        "  if root_type == 'Triple':\n",
        "    triple_counter += 1\n",
        "    if triple_counter == 1:\n",
        "      csv_triple_matrix = csv_row\n",
        "    else:\n",
        "      csv_triple_matrix = np.vstack((csv_triple_matrix,csv_row))\n",
        "\n",
        "    if len(csv_row) == triple_correct_length:\n",
        "      triple_correct_landmarks += 1\n",
        "\n",
        "print(\"csv single matrix\",csv_single_matrix)\n",
        "print(\"csv double matrix\",csv_double_matrix)\n",
        "print(\"csv triple matrix\",csv_triple_matrix)\n",
        "\n",
        "print(\"csv single matrix shape\",csv_single_matrix.shape)\n",
        "print(\"csv double matrix shape\",csv_double_matrix.shape)\n",
        "print(\"csv triple matrix shape\",csv_triple_matrix.shape)\n",
        "\n",
        "print(\"landmarks[051 copy 8_Single_2]\",landmarks['051 copy 8_Single_2'])\n",
        "\n",
        "print(\"single correct landmarks: \",single_correct_landmarks ,\" / \",single_counter)\n",
        "print(\"double correct landmarks: \",double_correct_landmarks ,\" / \",double_counter)\n",
        "print(\"triple correct landmarks: \",triple_correct_landmarks ,\" / \",triple_counter)\n",
        "\n",
        "\n",
        "# amount_of_teeth = len(list(landmarks.keys()))\n",
        "amount_of_teeth = single_correct_landmarks + double_correct_landmarks + triple_correct_landmarks\n",
        "amount_of_errors = len(list(error_x_rays.keys()))\n",
        "print(\"THESE ARE THE ERRORS LISTED!!!!!!!!!!!!!!!!!!!\",list(error_x_rays.keys()))\n",
        "print(\"amount of error x rays\",amount_of_errors)\n",
        "print(\"amount of usable x rays\",amount_of_teeth)\n",
        "print(\"percentage of error x rays: \",(amount_of_errors*100)/(amount_of_errors+amount_of_teeth),\"%\")\n",
        "print(\"types of errors\",Count_Dictionary_Values(error_x_rays))\n",
        "\n",
        "\n",
        "\n",
        "#save the csv files\n",
        "pd.DataFrame(csv_single_matrix).to_csv(\"individual_teeth/single_roots.csv\")\n",
        "pd.DataFrame(csv_double_matrix).to_csv(\"individual_teeth/double_roots.csv\")\n",
        "pd.DataFrame(csv_triple_matrix).to_csv(\"individual_teeth/triple_roots.csv\")\n",
        "\n",
        "#note that the printed reasoning may not be the only error that the image has it could have numeroud,\n",
        "#but only have one written (to avoid repetition)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8ggUrKxND3M"
      },
      "source": [
        "print(\"error x rays\",error_x_rays)\n",
        "#1079_Single_1 is example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Wu9w2avXrY2"
      },
      "source": [
        "!zip -r individual_teeth.zip individual_teeth"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}